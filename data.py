"""
Created: July 2021 
Last modified: October 2022 
Author: Veronica Saz Ulibarrena 
Description: Process of dataset, load dataset, normalization
"""
import os, sys
import h5py
import numpy as np
import json
import joblib
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer
import pandas as pd


def load_json(path):
    """
    load_json: load json configuration file 
    INPUTS:
        path: path of file
    OUTPUTS:
        config_data: dictionary with json data
    """
    with open(path) as jsonFile:
        config_data = json.load(jsonFile)
    jsonFile.close()

    return config_data

def get_dataset(save_dir, name):
    """
    get_dataset: load dataset with data generated by generate_training_data.py
    INPUTS:
        save_dir: path of the directory finishing without /
        name: name of the file
    OUTPUTS:
        config_data: dictionary with json data
    """
    path = os.path.join(save_dir, name)
    with h5py.File(path, 'r') as h5f:
        data = {}
        for dset in h5f.keys():
            data[dset] = h5f[dset][()]

    print("Successfully loaded data from {}".format(path))
    return data

def correct_data(data, n_particles):
    """
    correct_data: eliminate samples that have escaped
    INPUTS:
        data: dataset with samples in columns and inputs in row
        n_particles: number of particles in the system
        n_inputs:
    OUTPUTS:
        data2: corrected dataset
        new number of samples
    """
    data2 = np.copy(data)

    # Eliminate if the norm r has gone out of range (indication of escape of planet)
    for i in range(n_particles):
        r = np.linalg.norm(data2[:, i*4+1: i*4+4], axis = 1)
        r_mean = np.mean(r)
        r_std = np.std(r)
        idx = np.where(r > (r_mean + 1.5*r_std)) 
        data2 = np.delete(data2, idx, axis = 0)

    return data2, np.shape(data2)[0]

def calculate_energy(data, m, particles):
    """
    calculate_energy: calculate total energy of a sample
    INPUTS:
        data: dataset. samples x inputs
        m: masses of each particle
        particles: number of particles in the system
    OUTPUTS:
        H: total energy 
    """
    T = 0
    U = 0
    G = 1
    r = data[:, 0:3]
    v = data[:, 3:]
    # print(m, r, v)
    for i in range(particles):
        T += m[i] * np.linalg.norm(v[i,:])**2 / 2 
        for j in range(i+1, particles):
            U -= G * m[i] * m[j] / np.linalg.norm(r[j,:]- r[i,:])
    H = T + U
    return H

def rot_matrix(alpha, axis):
    """
    rot_matrix: creation of rotation matrix
    INPUTS: 
        alpha: angle
        axis: axis for the rotation 'x', 'y', 'z'
    OUTPUTS:
        matrix: rotation matrix
    """
    if axis == 'x':
        indx = 0
    elif axis == 'y':
        indx = 1
    elif axis == 'z':
        indx = 2
    
    matrix = np.zeros((3, 3))

    # Main diagonal
    matrix[indx, indx] = 1
    matrix[(indx +1)%3, (indx +1)%3 ] = np.cos(alpha)
    matrix[(indx +2)%3, (indx +2)%3 ] = np.cos(alpha)
    # Sine terms
    matrix[(indx +1)%3, (indx +2)%3] = -np.sin(alpha)
    matrix[(indx +2)%3, (indx +1)%3] = np.sin(alpha)

    return matrix

def rot_invariance(data):
    """
    rot_invariance: make data rotationaly invariant. (NOT TESTED)
    INPUTS:
        data: initial data
    OUTPUTS:     
        data_rot: rotated data for the 1st planet to be in the x axis (y=0, z=0)
        angles_rot: angles rotated needed to undo
    """
    data_rot = np.zeros(np.shape(data))
    angles_rot = np.zeros((len(data), 2))

    x = data[:, 0]
    y = data[:, 1]
    z = data[:, 2]

    alpha = np.arctan2(y, x)
    beta = np.arctan2(z, np.linalg.norm([x,y], axis = 0))
    angles_rot = np.hstack((alpha, beta))
    for i in range(len(data)):
        # features
        mat = np.matmul(rot_matrix(beta[i], "y"), rot_matrix(-alpha[i], "z")) 
        for j in range(np.shape(data)[1]//3):
            data_rot[i, j*3:j*3+3] = mat.dot(data[i, j*3:j*3+3])

    return data_rot, angles_rot

def rot_invariance_inverse(data, angles):
    """
    rot_invariance_inverse: undo rotationaly invariant data. (NOT TESTED)
    INPUTS:
        data: rotated data
        angles: rotated angles
    OUTPUTS:     
        data_rot: original data
    """
    data_rot = np.zeros(np.shape(data))
    if angles.ndim == 1:
       angles = np.reshape(angles, (-1, 2))
    alpha = angles[:, 0]
    beta = angles[:, 1]
    
    for i in range(len(data)):
        mat = np.matmul(np.transpose(rot_matrix(-alpha[i], "z")), np.transpose(rot_matrix(beta[i], "y"))) 
        for j in range(np.shape(data)[1]//3):
            data_rot[i, j*3:j*3+3] = mat.dot(data[i, j*3:j*3+3])

    return data_rot

def get_traintest_data(data, config_NN, direcName):
    """
    get_traintest_data: make input and output data from dataset. 
    INPUTS:
        data: generated dataset by generate_training_data.py
        config_NN: configuration from config_ANN.json
    OUTPUTS:
        data2: dictionary with 
                training dataset inputs 'coords'
                training dataset outputs 'dcoords'
                test dataset inputs 'coords'
                test dataset outputs 'dcoords'
        input_dim: number of inputs
    """
    # Load config
    config = load_json("./config_data.json")

    simulations = np.shape(data['coords'])[0]
    simulations_test = np.shape(data['test_coords'])[0]
    particles = np.shape(data['coords'])[1] -1 # Not include central body
    dimension = np.shape(data['coords'])[2] //2 # Only include position, not momentum

    coords = np.zeros((simulations, particles*(dimension+1))) # +1 to include mass
    dcoords = np.zeros((simulations, particles*dimension))
    test_coords = np.zeros((simulations_test, particles*(dimension+1)))
    test_dcoords = np.zeros((simulations_test, particles*dimension))

    # Use positions 
    for sim in range(simulations):  
        data['mass'][sim, -1] *= 1e8 # make same order of magnitude as planets
        coords[sim] = np.hstack(( np.reshape(data['mass'][sim, 1:], (-1,1)), data['coords'][sim, 1:,0:3])).flatten()
        dcoords[sim] = data['dcoords'][sim, 1:, 3:].flatten() # to get a

    for sim in range(simulations_test):
        data['test_mass'][sim, -1] *= 1e8
        test_coords[sim] = np.hstack(( np.reshape(data['test_mass'][sim, 1:], (-1,1)), data['test_coords'][sim, 1:,0:3])).flatten()
        test_dcoords[sim] = data['test_dcoords'][sim, 1:, 3:].flatten()

    input_dim = (dimension+1)*particles
    output_dim = dimension * particles

    # Shuffle data
    data_shuffle = np.hstack(( coords, dcoords))
    data_shuffle_test = np.hstack(( test_coords, test_dcoords))
    np.random.shuffle(data_shuffle)
    np.random.shuffle(data_shuffle_test)

    # Eliminate unstable orbits. If any body escapes
    print("Initial size of dataset: ", np.shape(data_shuffle))
    data_shuffle, simulations = correct_data(data_shuffle, particles)
    test_data_shuffle, simulations_test = correct_data(data_shuffle_test, particles)
    print("Size of dataset after correcting: ", np.shape(data_shuffle))


    # TODO: still not working with loss function
    if config['standardize'] == True:
        # Inputs and outputs separately, easier for prediction later
        data_shuffle[:, 0:input_dim] = standardize_andfit(data_shuffle[:, 0:input_dim], './dataset/', typeI = "I")
        data_shuffle[:, input_dim:] = standardize_andfit(data_shuffle[:, input_dim:], './dataset/', typeI = "O")

        test_data_shuffle[:, 0:input_dim] = standardize(test_data_shuffle[:, 0:input_dim], './dataset/', typeI = "I", inverse = 'False')
        test_data_shuffle[:, input_dim:] = standardize(test_data_shuffle[:, input_dim:], './dataset/', typeI = "O", inverse = 'False')
    
    # Take only a fraction of the data
    data_fraction = config['data_frac'] # Reduce the amount of data to make it lighter
    size_train = int(simulations * data_fraction)

    train_data = data_shuffle[0: size_train, :]
    test_data = test_data_shuffle

    data_2 = dict()
    data_2['coords'] = train_data[:, 0:input_dim]
    data_2['dcoords'] = train_data[:, input_dim:]
    data_2['test_coords'] = test_data[:, 0:input_dim]
    data_2['test_dcoords'] = test_data[:, input_dim:]

    # Save to file
    path_dataset = direcName
    with h5py.File(os.path.join(path_dataset, 'train_test_processed.h5'), 'w') as h5f:
        for dset in data_2.keys():
            h5f.create_dataset(dset, data=data_2[dset], compression="gzip")
    return data_2, input_dim


def standardize_andfit(data, path, typeI = "I"):
    """
    standardize_and_fit: fit dataset and standardize
    INPUTS: 
        data: data to fit and standardize
        path: path to save normalization 
        typeI: 
            "I": inputs are given
            "O": outputs are given
    OUTPUTS:
        data_2: normalized data
    """
    # Type of normalization
    # scaler = MinMaxScaler()
    # scaler = StandardScaler()
    # scaler = RobustScaler(with_centering = True, with_scaling = True, unit_variance = True)
    scaler = QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal')
    
    fit = scaler.fit(data)
    data_2 = scaler.transform(data)

    # save to file
    joblib.dump(fit, path + "std_scale_"+ typeI+ ".bin", compress=True)
    return data_2

def standardize(data, path, inverse = False, typeI = "I"):
    """
    standardize: normalize data using a file from standardize_andfit
    INPUTS: 
        data: data to standardize
        path: path of saved normalization 
        inverse: 
            True: inverse standardization
            False: standardize
        typeI: 
            "I": inputs are given
            "O": outputs are given
    OUTPUTS:
        data_2: normalized data (if inverse = False), original data (inverse = True)
    """
    scaler = joblib.load(path + "std_scale_"+ typeI+ ".bin" )
    if inverse == False:
        data_2 = scaler.transform(data)
    else:
        data_2 = scaler.inverse_transform(data)

    return data_2
